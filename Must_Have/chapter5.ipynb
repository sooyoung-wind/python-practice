{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. [분류와 회귀](#분류와-회귀)  \n",
    "    1.1. [분류](#분류)  \n",
    "    1.2. [회귀](#회귀)  \n",
    "2. [분류 평가지표](#분류-평가지표)  \n",
    "    2.1. [오차 행렬(confusion matrix)](#오차-행렬confusion-matrix)  \n",
    "    2.2. [로그 손실](#로그-손실)  \n",
    "    2.3. [ROC 곡선과 AUC](#ROC-곡선과-AUC)  \n",
    "3. [데이터 인코딩](#데이터-인코딩)  \n",
    "    3.1. [레이블 인코딩](#레이블-인코딩)  \n",
    "    3.2. [오디널 인코딩](#오디널-인코딩)  \n",
    "    3.3. [원핫 인코딩](#원핫-인코딩)  \n",
    "4. [피쳐 스케일링](#피쳐-스케일링)  \n",
    "    4.1. [min-max 정규화(normalization)](#min-max-정규화normalization)  \n",
    "    4.2. [표준화(standardization)](#표준화standardization)  \n",
    "5. [교차 검증](#교차-검증)  \n",
    "    5.1. [K 폴드 교차 검증](#K-폴드-교차-검증)  \n",
    "    5.2. [층화 K 폴드 교차 검증](#층화-K-폴드-교차-검증)  \n",
    "6. [주요 머신러닝 모델](#주요-머신러닝-모델)  \n",
    "    6.1. [선형 회귀 모델](#선형-회귀-모델)  \n",
    "    6.2. [로지스틱 회귀 모델](#로지스틱-회귀-모델)  \n",
    "    6.3. [결정 트리](#결정-트리)  \n",
    "    6.4. [앙상블 학습](#앙상블-학습)  \n",
    "    6.5. [랜덤 포레스트](#랜덤-포레스트)  \n",
    "    6.6. [XGBoost](#XGBoost)  \n",
    "    6.7. [LightGBM](#LightGBM)  \n",
    "7. [하이퍼 파라미터 최적화](#하이퍼-파라미터-최적화)  \n",
    "    7.1. [그리드서치](#그리드서치)  \n",
    "    7.2. [랜덤서치](#랜덤서치)  \n",
    "    7.3. [베이지안 최적화](#베이지안-최적화) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"분류와-회귀\"></a>\n",
    "# 분류와 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "캐글 경진대회는 대부분 분류나 회귀 문제를 다룬다. 타깃값이 범주형 데이터면 분류문제, 수치형이면 회귀 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"분류\"></a>\n",
    "## 분류\n",
    "책의 내용을 피처, IT 도서을 타깃을 비유함. 책을 분류하는 방법에 대한 설명\n",
    "\n",
    "### 5.1.1 분류\n",
    "이진 분류(binary clasification) : 타깃값이 2개인 분류\n",
    "다중 분류(multicalss classification) : 타깃값이 3개 이상인 분류\n",
    "\n",
    "### 5.1.2 회귀\n",
    "독립변수(independent variable) : 영향을 미치는 변수\n",
    "종속변수(dependent variable) : 영향을 받는 변수\n",
    "회귀란 독립변수와 종속변수 간 관계를 모델링하는 방법이다.\n",
    "\n",
    "선형회귀(simple linear regression) : y = ax + b\n",
    "다중 선형회귀(multiple linear regression) : y = ax + by + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"회귀\"></a>\n",
    "## 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "true = np.array([1,2,3,4,5,7,8,6,4,4,3])\n",
    "pred = np.array([1,2,3,4,5,7,2,6,4,7,3])\n",
    "\n",
    "MAE = mean_absolute_error(true, pred)\n",
    "MSE = mean_squared_error(true, pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MSLE = mean_squared_log_error(true, pred)\n",
    "RMSLE = np.sqrt(MSLE)\n",
    "R2 = r2_score(true, pred)\n",
    "\n",
    "print(f'MAE:\\t {MAE:.4f}')\n",
    "print(f'MSE:\\t {MSE:.4f}')\n",
    "print(f'RMSE:\\t {RMSE:.4f}')\n",
    "print(f'MSLE:\\t {MSLE:.4f}')\n",
    "print(f'RMSLE:\\t {RMSLE:.4f}')\n",
    "print(f'R2:\\t {R2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"분류-평가지표\"></a>\n",
    "# 분류 평가지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"오차-행렬confusion-matrix\"></a>\n",
    "## 오차 행렬(confusion matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"로그-손실\"></a>\n",
    "## 로그 손실"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ROC-곡선과-AUC\"></a>\n",
    "## ROC 곡선과 AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"데이터-인코딩\"></a>\n",
    "# 데이터 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"레이블-인코딩\"></a>\n",
    "## 레이블 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "fruits = ['banana', 'berry', 'blueberry']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "fruits_label_encoded = label_encoder.fit_transform(fruits)\n",
    "\n",
    "print('레이블 인코딩 적용 후 데이터:', fruits_label_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"오디널-인코딩\"></a>\n",
    "## 오디널 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"원핫-인코딩\"></a>\n",
    "## 원핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "fruits = ['banana', 'berry', 'blueberry']\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "fruits_onehot_encoded = onehot_encoder.fit_transform(fruits)\n",
    "\n",
    "print('원-핫 인코딩 적용 후 데이터:', fruits_onehot_encoded.toarry())\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fruits_get_dummy = pd.get_dummies(fruits)\n",
    "\n",
    "print('더미 적용 후 데이터:', fruits_get_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"피쳐-스케일링\"></a>\n",
    "# 피쳐 스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"min-max-정규화normalization\"></a>\n",
    "## min-max 정규화(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "height_weight_dict = {'키': [1.7, 1.5, 1.8], '몸무게': [75, 55, 60]}\n",
    "df = pd.DataFrame(height_weight_dict, index=['광일', '혜성', '덕수'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df)\n",
    "df_scaled = scaler.transform(df)\n",
    "\n",
    "print(df)\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"표준화standardization\"></a>\n",
    "## 표준화(standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "height_weight_dict = {'키': [1.7, 1.5, 1.8], '몸무게': [75, 55, 60]}\n",
    "df = pd.DataFrame(height_weight_dict, index=['광일', '혜성', '덕수'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df_scaled = scaler.transform(df)\n",
    "\n",
    "print(df)\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"교차-검증\"></a>\n",
    "# 교차 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"K-폴드-교차-검증\"></a>\n",
    "## K 폴드 교차 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, valid in folds.split(data):\n",
    "    print(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"층화-K-폴드-교차-검증\"></a>\n",
    "## 층화 K 폴드 교차 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "data = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, valid in folds.split(iris, iris['species']):\n",
    "    print(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "data = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "iris = sns.load_dataset('iris')\n",
    "y = iris['species']\n",
    "iris = iris.drop(columns=\"species\")\n",
    "print(y)\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, valid in folds.split(iris, y):\n",
    "    print(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"주요-머신러닝-모델\"></a>\n",
    "# 주요 머신러닝 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"선형-회귀-모델\"></a>\n",
    "## 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "w0 = 5\n",
    "w1 = 2\n",
    "noise = np.random.randn(100, 1)\n",
    "\n",
    "x = 4* np.random.rand(100, 1)\n",
    "y = w1*x+w0+noise\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(x,y)\n",
    "\n",
    "print('w0 : ', linear_reg_model.intercept_)\n",
    "print('w1 : ', linear_reg_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_reg_model.predict(x)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"로지스틱-회귀-모델\"></a>\n",
    "## 로지스틱 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "df[\"label\"] = breast_cancer.target\n",
    "print(df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step1) train / test 으로 나누기\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# 모델 만들기\n",
    "majority_class = train[\"label\"].mode()[0]\n",
    "\n",
    "# 기준모델의 정확도 계산을 위한 데이터 생성\n",
    "y_pred = [majority_class] * len(test)\n",
    "\n",
    "# validation 데이터셋에 대한 정확도 확인\n",
    "print(\"최빈 클래스: \", majority_class)\n",
    "print(\"validation 데이터셋 정확도: \", accuracy_score(test[\"label\"], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_train = train['label']\n",
    "X_train = train.drop(columns='label')\n",
    "y_test = test['label']\n",
    "X_test = test.drop(columns='label')\n",
    "\n",
    "# 모델 생성 및 학습 시키기\n",
    "logistic = LogisticRegression(max_iter=5000) # iter 초과시 증가\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"validation 데이터셋 정확도\", logistic.score(X_test, y_test))\n",
    "print(\"validation 데이터셋의 타겟 확률\", logistic.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"결정-트리\"></a>\n",
    "## 결정 트리\n",
    "\n",
    "* criterion : 불순도 측정 지표 [default : 'gini', 'entropy']\n",
    "* max_depth : 트리 최대 깊이 [default : None]\n",
    "    * min_samples_split과 같은 옵션이 없으면 기본으로 불순도 0까지 분할\n",
    "* min_samples_split : 노드 분할에 필요한 최소 데이터수 [default : 2]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* min_samples_leaf : 말단 노드가 되기 위한 최소 데이터수 [default : 1]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* max_features : 분할에 사용할 피쳐수 [default : None]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "    * 'auto', 'sqrt', 'log2' 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(breast_cancer['data'],\n",
    "                                breast_cancer['target'],\n",
    "                                stratify=breast_cancer['target'],\n",
    "                                test_size=0.2,\n",
    "                                random_state=0\n",
    "                                )\n",
    "                    \n",
    "decisiontree = DecisionTreeClassifier(random_state=0)\n",
    "decisiontree.fit(X_train, y_train)\n",
    "\n",
    "accuracy = decisiontree.score(X_test, y_test)\n",
    "\n",
    "print(f'결정 트리 정확도 : {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"앙상블-학습\"></a>\n",
    "## 앙상블 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#load dataset\n",
    "cancer_dataset = load_breast_cancer()\n",
    "\n",
    "# cancer_dataset 그냥 찍어보니 이상하게 나옴.. dataframe화 해줘야 헸음. \n",
    "cancer_dataset_df = pd.DataFrame(cancer_dataset.data, columns=cancer_dataset.feature_names)\n",
    "cancer_dataset_df.head()\n",
    "\n",
    "\n",
    "# dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_dataset.data, cancer_dataset.target, test_size=0.2, random_state=121)\n",
    "\n",
    "\n",
    "#weak learners: logistic regression, KNN\n",
    "logistic_regression = LogisticRegression()\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "#votinng ensemble with these two weak learners\n",
    "voting_ensemble = VotingClassifier(estimators=[(\"LogisticRegression\", logistic_regression), (\"KNN\", KNN)],\n",
    "                                  voting = 'soft')\n",
    "\n",
    "\n",
    "# voting_ensemble model train/val/test\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "y_pred = voting_ensemble.predict(X_test)\n",
    "\n",
    "print(\"voting 분류기 정확도 {0:.4f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"랜덤-포레스트\"></a>\n",
    "## 랜덤 포레스트\n",
    "\n",
    "* n_estimators : 랜덤 포레스트에 사용될 트리의 수  [default : 100]\n",
    "* criterion : 불순도 측정 지표 [default : 'gini', 'entropy']\n",
    "* max_depth : 트리 최대 깊이 [default : None]\n",
    "    * min_samples_split과 같은 옵션이 없으면 기본으로 불순도 0까지 분할\n",
    "* min_samples_split : 노드 분할에 필요한 최소 데이터수 [default : 2]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* min_samples_leaf : 말단 노드가 되기 위한 최소 데이터수 [default : 1]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* max_features : 분할에 사용할 피쳐수 [default : None]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "    * 'auto', 'sqrt', 'log2' 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(breast_cancer['data'],\n",
    "                                breast_cancer['target'],\n",
    "                                stratify=breast_cancer['target'],\n",
    "                                test_size=0.2,\n",
    "                                random_state=0\n",
    "                                )\n",
    "                    \n",
    "decisiontree = RandomForestClassifier(random_state=0)\n",
    "decisiontree.fit(X_train, y_train)\n",
    "\n",
    "accuracy = decisiontree.score(X_test, y_test)\n",
    "\n",
    "print(f'결정 트리 정확도 : {accuracy:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"XGBoost\"></a>\n",
    "## XGBoost\n",
    "\n",
    "for XGBoost() class\n",
    "* booster : 부스팅 알고리즘 [default : 'gbtree', 'dart', 'gblinear']\n",
    "    * gblinear : 선형 모델(성능 하)\n",
    "    * dart : 드롭아웃 적용한 gbtree\n",
    "* objective : 훈련 목적 [default : 'reg:squarederror']\n",
    "    * 회귀에서 'reg:squarederror'\n",
    "    * 확룰성 이진분류에서 'binary:logistic'\n",
    "    * softmax 사용 다중분류에서 'multi:softmax'\n",
    "    * 확률값을 구하는 다중분류에서 'multi:softprob'\n",
    "* eta : 학습률(단계별 가중치) [default : 0.3, 0~1, normally 0.001~0.1]\n",
    "* max_depth : 트리의 최대 깊이 [default : 6, normally 3~10]\n",
    "* subsample : 개별 트리에 사용할 데이터 샘플링 비율 [default : 1, normally 0.6~1]\n",
    "* colsample_bytree : 트리 훈련에 사용하는 피쳐 샘플링 비율 [default : 1, normally 0.6~1]\n",
    "* alpha : L1(lasso) 규제 값 [default : 0]\n",
    "* reg_lambda : L2(Ridge) 규제 값 [default : 1]\n",
    "* gamma(min_split_loss) : leaf node가 분할 하기위한 최소 loss[default : 0, 0~$\\infty$]\n",
    "* min_child_weight : 과적합 방지 값 [default : 1, 0~$\\infty$]\n",
    "* scale_pos_weight : 불균형 데이터 가중치 조정 값 [default : 1]\n",
    "    * 주로 $음성타깃 \\over 양성타깃$ 으로 설정\n",
    "    \n",
    "for train() method\n",
    "* params : dict type 하이퍼 파라미터 목록\n",
    "* dtrain : 훈련 데이터셋\n",
    "* num_boost_round : 부스팅 반복 횟수 [default : 10]\n",
    "    * 성능, 과적합, 시간의 상관관계가 있음\n",
    "* evals : 검증용 데이터 셋 [default : []]\n",
    "* feval : 검증용 평가지표 evals사용시 활용 [default : None]\n",
    "* maximize : feval이 높은게 좋은가? [True, False]\n",
    "* early_stopping_rounds : 조기종료 옵션 [default : None, int]\n",
    "    * eta가 크면 줄이고 작으면 키움\n",
    "    * evals가 필요함\n",
    "    * 옵션의 숫자만큼 반복하고 다음번으로 반복한 결과와 비교해 성능이 감소하면 종료\n",
    "* verbose_eval : 성능 점수 로그 설정 [default : True, False, int]\n",
    "    * 설정된 횟수만큼 반복후 성능지표를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"LightGBM\"></a>\n",
    "## LightGBM\n",
    "\n",
    "for LightGBM() class\n",
    "* boosting_type : 부스팅 알고리즘 [default : 'gbdt', 'dart', 'goss','rf]\n",
    "    * g\n",
    "* objective : 훈련 목적 [default : 'regression']\n",
    "    * 회귀에서 'regression'\n",
    "    * 이진분류에서 'binary'\n",
    "    * 다중분류에서 'multiclass'\n",
    "* learning_rate(eta) : 학습률(단계별 가중치) [default : ]\n",
    "* num_leaves : leaf node 갯수 커지면 과적합 위험 [default : 31]\n",
    "* max_depth : 트리의 최대 깊이 [default : -1]\n",
    "* bagging_fraction(subsample) : 개별 트리에 사용할 데이터 샘플링 비율 [default : not 0]\n",
    "    * g\n",
    "* feature_fraction(colsample_bytree) : 트리 훈련에 사용하는 피쳐 샘플링 비율 [default : ]\n",
    "    * g\n",
    "* lambda_l1(reg_alpha) : L1(lasso) 규제 값 [default : 0]\n",
    "* lambda_l2(reg_lambda) : L2(Ridge) 규제 값 [default : 0]\n",
    "* min_child_samples : leaf node가 되기위한 최소 데이터 수 [default : 20]\n",
    "* min_child_weight : 과적합 방지 값 [default : $1e^{-3}$, 0~$\\infty$]\n",
    "* bagging_freq : 배깅 수행 빈도 [default : 0]\n",
    "    * 몇번의 iter 마다 배깅을 할지 결정\n",
    "* force_row_wise : 메모리 효율 증가 [default : False, True]\n",
    "\n",
    "for train() method\n",
    "* params : dict type 하이퍼 파라미터 목록\n",
    "* train_set : 훈련 데이터셋\n",
    "* num_boost_round : 부스팅 반복 횟수 [default : 10]\n",
    "    * 성능, 과적합, 시간의 상관관계가 있음\n",
    "* valid_sets : 검증용 데이터 셋 [default : None]\n",
    "* feval : 검증용 평가지표 evals사용시 활용 [default : None]\n",
    "* categorical_feature : 범주형 데이터 지정옵션\n",
    "* early_stopping_rounds : 조기종료 옵션 [default : None, int]\n",
    "    * eta가 크면 줄이고 작으면 키움\n",
    "    * evals가 필요함\n",
    "    * 옵션의 숫자만큼 반복하고 다음번으로 반복한 결과와 비교해 성능이 감소하면 종료\n",
    "* verbose_eval : 성능 점수 로그 설정 [default : True, False, int]\n",
    "    * 설정된 횟수만큼 반복후 성능지표를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"하이퍼-파라미터-최적화\"></a>\n",
    "# 하이퍼 파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"그리드서치\"></a>\n",
    "## 그리드서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"랜덤서치\"></a>\n",
    "## 랜덤서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"베이지안-최적화\"></a>\n",
    "## 베이지안 최적화\n",
    "\n",
    "* init_points : 랜덤 탐색의 스텝 횟수\n",
    "* n_iter : 베이지안 실행 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization\n",
    "\n",
    "param_bounds = {'max_depth': (4, 8),\n",
    "'subsample': (0.6, 0.9),\n",
    "'colsample_bytree': (0.7, 1.0),\n",
    "'min_child_weight': (5, 7),\n",
    "'gamma': (8, 11),\n",
    "'reg_alpha': (7, 9),\n",
    "'reg_lambda': (1.1, 1.5)}\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def eval_function(max_depth, subsample, colsample_bytree, min_child_weight, gamma, reg_alpha, reg_lambda):\n",
    "  param_bounds = {'max_depth': max_depth,\n",
    "                  'subsample': subsample,\n",
    "                  'colsample_bytree': colsample_bytree,\n",
    "                  'min_child_weight': min_child_weight,\n",
    "                  'gamma': gamma,\n",
    "                  'reg_alpha': reg_alpha,\n",
    "                  'reg_lambda': reg_lambda}\n",
    "  xgb_model = xgb.train(params=params, dtrain=train, num_boost_round=2000, evals=[(valid, 'valid')], maximize=True, early_stopping_round=200)\n",
    "  best_iter = xgb_model.best_iteration\n",
    "  preds = xgb_model.predict(valid, iteration_range=(0, best_iter))\n",
    "  score = roc_auc_score(y_valid, preds)\n",
    "\n",
    "  return score\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizer = BayesianOptimization(f=eval_function, pbounds=param_bounds)\n",
    "\n",
    "optimizer.maximize(init_points=3, n_iter=10)\n",
    "\n",
    "max_params = optimzier.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b363c58d1093e891c4eaaf9273db69d0f7db3f9909d89c6197aa3afcc07f476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. [분류와 회귀](#분류와-회귀)  \n",
    "    1.1. [분류](#분류)  \n",
    "    1.2. [회귀](#회귀)  \n",
    "2. [분류 평가지표](#분류-평가지표)  \n",
    "    2.1. [오차 행렬(confusion matrix)](#오차-행렬confusion-matrix)  \n",
    "    2.2. [로그 손실](#로그-손실)  \n",
    "    2.3. [ROC 곡선과 AUC](#ROC-곡선과-AUC)  \n",
    "3. [데이터 인코딩](#데이터-인코딩)  \n",
    "    3.1. [레이블 인코딩](#레이블-인코딩)  \n",
    "    3.2. [오디널 인코딩](#오디널-인코딩)  \n",
    "    3.3. [원핫 인코딩](#원핫-인코딩)  \n",
    "4. [피쳐 스케일링](#피쳐-스케일링)  \n",
    "    4.1. [min-max 정규화(normalization)](#min-max-정규화normalization)  \n",
    "    4.2. [표준화(standardization)](#표준화standardization)  \n",
    "5. [교차 검증](#교차-검증)  \n",
    "    5.1. [K 폴드 교차 검증](#K-폴드-교차-검증)  \n",
    "    5.2. [층화 K 폴드 교차 검증](#층화-K-폴드-교차-검증)  \n",
    "6. [주요 머신러닝 모델](#주요-머신러닝-모델)  \n",
    "    6.1. [선형 회귀 모델](#선형-회귀-모델)  \n",
    "    6.2. [로지스틱 회귀 모델](#로지스틱-회귀-모델)  \n",
    "    6.3. [결정 트리](#결정-트리)  \n",
    "    6.4. [앙상블 학습](#앙상블-학습)  \n",
    "    6.5. [랜덤 포레스트](#랜덤-포레스트)  \n",
    "    6.6. [XGBoost](#XGBoost)  \n",
    "    6.7. [LightGBM](#LightGBM)  \n",
    "7. [하이퍼 파라미터 최적화](#하이퍼-파라미터-최적화)  \n",
    "    7.1. [그리드서치](#그리드서치)  \n",
    "    7.2. [랜덤서치](#랜덤서치)  \n",
    "    7.3. [베이지안 최적화](#베이지안-최적화) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"분류와-회귀\"></a>\n",
    "# 분류와 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "캐글 경진대회는 대부분 분류나 회귀 문제를 다룬다. 타깃값이 범주형 데이터면 분류문제, 수치형이면 회귀 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"분류\"></a>\n",
    "## 분류\n",
    "책의 내용을 피처, IT 도서을 타깃을 비유함. 책을 분류하는 방법에 대한 설명\n",
    "\n",
    "### 5.1.1 분류\n",
    "이진 분류(binary clasification) : 타깃값이 2개인 분류\n",
    "다중 분류(multicalss classification) : 타깃값이 3개 이상인 분류\n",
    "\n",
    "### 5.1.2 회귀\n",
    "독립변수(independent variable) : 영향을 미치는 변수\n",
    "종속변수(dependent variable) : 영향을 받는 변수\n",
    "회귀란 독립변수와 종속변수 간 관계를 모델링하는 방법이다.\n",
    "\n",
    "선형회귀(simple linear regression) : y = ax + b\n",
    "다중 선형회귀(multiple linear regression) : y = ax + by + c\n",
    "\n",
    "### 자주 사용하는 회귀 평가지표\n",
    "- MAE : 평균절대오차\n",
    "- MSE : 평균제곱오차\n",
    "- RMSE : 평균제곱근 오차\n",
    "- MSLE : Mean Squared Log Error\n",
    "- RMSLE : Root Mean Squared Log Error\n",
    "- R^2 : 결정계수\n",
    "\n",
    "RMSE vs. RMSLE\n",
    "1. 아웃라이어에 강건하다. : 이상치 또는 아웃라이어가 있더라도 값의 변동폭이 크지 않다.\n",
    "2. 상대적 Error 측정 : 상대적 및 절대적 차이에 의해 RMSE는 변화하지만, RMSLE은 예측값과 실제값의 상대적 error를 측정한다.\n",
    " - 단, RMSLE의 한계는 예시로 돈 관련 문제에 적용할 때 1억과 100억의 오차와 1원과 100원의 오차 의미가 퇴색하게 된다.\n",
    "3. Under Estimation에 큰 패널티를 부여한다.\n",
    " - RMSLE는 over estimation보다 under estimation에 더 큰 패널티를 부여한다. 즉, 예측값이 실제보다 작을 때 더 큰 패널티를 부여한다.  \n",
    " - 예를 배달에서 30분이 걸린다고 했는데 20분이 걸리는건 문제가 없지만, 20분이 걸린다고 했는데 30분이 걸린다면 문제가 있다고 볼 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|평가지표|수식|설명|\n",
    "|--|--|--|\n",
    "|MAE|$${1 \\over N} \\sum_{i=1}^N\\|y_i - \\hat{y_i}\\|$$||\n",
    "|MSE|$${1 \\over N} \\sum_{i=1}^N(y_i - \\hat{y_i})^2$$||\n",
    "|RMSE|$$\\sqrt{{1 \\over N} \\sum_{i=1}^N(y_i - \\hat{y_i})^2}$$||\n",
    "|MSLE|$${1 \\over N} \\sum_{i=1}^N(log(y_i+1)-log(\\hat{y_i}+1))^2$$|$$-\\infty$$를 방지하기 위하여 log에 +1을 취함|\n",
    "|RMSLE|$$\\sqrt{{1 \\over N} \\sum_{i=1}^N(log(y_i+1)-log(\\hat{y_i}+1))^2}$$|$$-\\infty$$를 방지하기 위하여 log에 +1을 취함|\n",
    "|$$R^2$$|$$ \\hat{\\sigma}^2 \\over {\\sigma}^2$$|1에 근첩하는게 정확도가 높음|  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"회귀\"></a>\n",
    "## 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:\t 0.8182\n",
      "MSE:\t 4.0909\n",
      "RMSE:\t 2.0226\n",
      "MSLE:\t 0.1298\n",
      "RMSLE:\t 0.3603\n",
      "R2:\t -0.0185\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "\n",
    "true = np.array([1,2,3,4,5,7,8,6,4,4,3])\n",
    "pred = np.array([1,2,3,4,5,7,2,6,4,7,3])\n",
    "\n",
    "MAE = mean_absolute_error(true, pred)\n",
    "MSE = mean_squared_error(true, pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MSLE = mean_squared_log_error(true, pred)\n",
    "RMSLE = np.sqrt(MSLE)\n",
    "R2 = r2_score(true, pred)\n",
    "\n",
    "print(f'MAE:\\t {MAE:.4f}')\n",
    "print(f'MSE:\\t {MSE:.4f}')\n",
    "print(f'RMSE:\\t {RMSE:.4f}')\n",
    "print(f'MSLE:\\t {MSLE:.4f}')\n",
    "print(f'RMSLE:\\t {RMSLE:.4f}')\n",
    "print(f'R2:\\t {R2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"분류-평가지표\"></a>\n",
    "# 분류 평가지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"오차-행렬confusion-matrix\"></a>\n",
    "## 오차 행렬(confusion matrix)\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>predict</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>positive</td>\n",
    "      <td>negative</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"2\">real</td>\n",
    "      <td>positive</td>\n",
    "      <td>TP</td>\n",
    "      <td>FN</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>negative</td>\n",
    "      <td>FP</td>\n",
    "      <td>TN</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "참 양성(True Positive, TP) : 양성으로 정답을 맞춘 경우  \n",
    "참 음성(True Negative, TN) : 음성으로 정답을 맞춘 경우  \n",
    "거짓 양성(False Positive, FP) : 양성으로 못 맞춘 경우  \n",
    "거짓 음성(False Negative, TN) : 음성으로 못 맞춘 경우  \n",
    "\n",
    "오차 행렬을 활용한 지표는 정확도, 정밀도, 재현율, F1 점수가 있다.  \n",
    "\n",
    "* 정확도(accuracy) : 정확도는 잘 쓰이지 않는다.  \n",
    "전체에서 True가 차지하는 비율  \n",
    "(*모델의 우수성을 따질때 잘 사용하지 않음)  \n",
    "$${TP+TN}\\over{TP+TN+FP+FN}$$\n",
    "\n",
    "* 정밀도(precision) : 정밀도는 음성을 양성으로 잘못 판단하면 문제가 발생하는 경우에 사용한다. 검출하기 원하는 상태를 보통 양성으로 정한다. 즉, 문제가 되는 상태를 양성, 정상인 상태를 음성이라고 본다.  \n",
    "Positive predict중의 True가 차지하는 비율  \n",
    "(*실제 negative가 positive로 측정되는것이 critical할때 사용)  \n",
    "$${TP}\\over{TP+FP}$$\n",
    "\n",
    "* 재현율(recall)(TPR) : 양성을 음성으로 잘못 판단하면 준제가 되는 경우에 사용한다.(예, 암진단)\n",
    "positive real중의 True가 차지하는 비율  \n",
    "(*실제 positive가 negative로 측정되는것이 critical할때 사용)  \n",
    "$${TP}\\over{TP+FN}$$\n",
    "\n",
    "* F1 점수(F1 score)  \n",
    "정밀도와 재현율의 조화평균으로 편중되지않은 균등한 모델을 만들때 사용  \n",
    "$${2}\\over{{{1} \\over {precision}}+{{1} \\over {recall}}}$$\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"로그-손실\"></a>\n",
    "## 로그 손실\n",
    "\n",
    "타겟을 확률적으로 예측시 사용되는 모델(0에 가까울 수록 좋음)  \n",
    "$$logloss = -{{1}\\over{N}}\\sum_{i=1}^N(y_ilog(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ROC-곡선과-AUC\"></a>\n",
    "## ROC 곡선과 AUC\n",
    "- ROC(Receiver Operating Characteristic)곡선은 참 양성 비율(TPR)에 대한 거짓 양성 비율(FPR)곡선이다.  \n",
    "- AUC(Area Under the Curve)는 ROC 곡선 아래 면적을 의미한다.  \n",
    "\n",
    "로그손실과 같이 예측값이 확률일때 사용하게 된다.  \n",
    "$$TNR = {TN}\\over{FP+TN}$$\n",
    "$$FPR = 1 - TNR$$  \n",
    "\n",
    "TNR은 특이도(specificity)라고도 불린다.\n",
    "<center>\n",
    "  <img\n",
    "    src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/512px-Roc_curve.svg.png\"\n",
    "    width=\"350\"\n",
    "    height=\"400\"\n",
    "  />\n",
    "</center>\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"데이터-인코딩\"></a>\n",
    "# 데이터 인코딩\n",
    "\n",
    "대표적인 데이터 인코딩에서는 **레이블 인코딩**과 **원-핫 인코딩**이 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"레이블-인코딩\"></a>\n",
    "## 레이블 인코딩  \n",
    "\n",
    "레이블 인코딩을 적용하면 원본 데이터의 값에 사전순으로 번호를 매깁니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 인코딩 적용 후 데이터: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "fruits = ['banana', 'berry', 'blueberry']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "fruits_label_encoded = label_encoder.fit_transform(fruits)\n",
    "\n",
    "print('레이블 인코딩 적용 후 데이터:', fruits_label_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"오디널-인코딩\"></a>\n",
    "## 오디널 인코딩  \n",
    "범주형 데이터를 숫자로 1대1 매칭해주는 방법(고차원 데이터)  \n",
    "데이터간의 상관관계가 있을 때 사용  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"원핫-인코딩\"></a>\n",
    "## 원핫 인코딩  \n",
    "범주형 데이터 value수 만큼 feature를 늘려서 해당하면 1 아니면 0을 나타냄  \n",
    "메모리를 아끼기 위해서 Compressed spares row로 변환됨  \n",
    "데이터간의 상관관계가 없을 때 사용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원-핫 인코딩 적용 후 데이터: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "fruits = ['banana', 'berry', 'blueberry']\n",
    "\n",
    "# 레이블 인코더, 원-핫 인코더 생성\n",
    "label_encoder = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "# 레이블 인코딩 적용(문자 데이터 -> 숫자 데이터)\n",
    "fruits_onehot_encoded = label_encoder.fit_transform(fruits)\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "# reshape의 -1 값은 다른 인자 값에 따라 의존적으로 변화는 값을 의미한다.  \n",
    "fruits_onehot_encoded = onehot_encoder.fit_transform(fruits_label_encoded.reshape(-1,1))\n",
    "\n",
    "print('원-핫 인코딩 적용 후 데이터: \\n', fruits_onehot_encoded.toarray())\n",
    "\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banana</th>\n",
       "      <th>berry</th>\n",
       "      <th>blueberry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   banana  berry  blueberry\n",
       "0    True  False      False\n",
       "1   False   True      False\n",
       "2   False  False       True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.get_dummies(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"피쳐-스케일링\"></a>\n",
    "# 피쳐 스케일링  \n",
    "\n",
    "피쳐들마다 범위가 다르기때문에 범위를 조절하는 기법  \n",
    "단, 트리 기반 모델(랜덤 포레시트, XGBoost, LightGBM 등)은 피처 스케일링이 필요 없다.  \n",
    "트리 기반 모델은 데이터의 크기보다는 대소 관계에 영향을 받기 때문이다. 피처 스케일링을 하더라도 데이터의 대소 관계에는 변함이 없다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"min-max-정규화normalization\"></a>\n",
    "## min-max 정규화(normalization)  \n",
    "\n",
    "0~1의 범위의 값으로 수치를 변화시키는것을 의미하며 아래의 식으로 변환됨  \n",
    "(outlier에 취약함)  \n",
    "$$x_{scaled} = {{x-x_{min}} \\over {x_{max}-x_{min}}}$$\n",
    "fit, transform을 따로 할때 음수 발현가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      키  몸무게\n",
      "광일  1.7   75\n",
      "혜성  1.5   55\n",
      "덕수  1.8   60\n",
      "[[0.66666667 1.        ]\n",
      " [0.         0.        ]\n",
      " [1.         0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "height_weight_dict = {'키': [1.7, 1.5, 1.8], '몸무게': [75, 55, 60]}\n",
    "df = pd.DataFrame(height_weight_dict, index=['광일', '혜성', '덕수'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df) # df 데이터에서 min max를 사용하여 동작\n",
    "df_scaled = scaler.transform(df) # 여기서는 예시로 df를 했지만 다른 데이터를 입력하면 앞에서 사용한 min max를 적용하는 부분\n",
    "# 예시\n",
    "# df_scaled2 = scaler.transform(df2)\n",
    "# df_scaled3 = scaler.transform(df3)\n",
    "\n",
    "print(df)\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"표준화standardization\"></a>\n",
    "## 표준화(standardization)  \n",
    "\n",
    "평균이 0, 분산이 1이 되게 피쳐를 조정(정규분포를 따르는 데이터는 표준화 스케일링을 적용하는게 좋다.)\n",
    "$$x_{scaled} = {{x-\\hat{x}}\\over{\\sigma}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      키  몸무게\n",
      "광일  1.7   75\n",
      "혜성  1.5   55\n",
      "덕수  1.8   60\n",
      "[[ 0.26726124  1.37281295]\n",
      " [-1.33630621 -0.98058068]\n",
      " [ 1.06904497 -0.39223227]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "height_weight_dict = {'키': [1.7, 1.5, 1.8], '몸무게': [75, 55, 60]}\n",
    "df = pd.DataFrame(height_weight_dict, index=['광일', '혜성', '덕수'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df_scaled = scaler.transform(df)\n",
    "\n",
    "print(df)\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"교차-검증\"></a>\n",
    "# 교차 검증  \n",
    "\n",
    "모델의 성능을 검증하는 방법으로 아래의 문제해결을 위해 사용됨  \n",
    "* 과적합  \n",
    "훈련, 테스트 데이터셋이 고정되면 과적합 가능성이 높아짐\n",
    "* 명확한 성능확인  \n",
    "경진대회에서는 제출제한 횟수, 실제에서는 실제데이터 적용의 문제로 명확하게 성능확인이 힘들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"K-폴드-교차-검증\"></a>\n",
    "## K 폴드 교차 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, valid in folds.split(data):\n",
    "    print(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"층화-K-폴드-교차-검증\"></a>\n",
    "## 층화 K 폴드 교차 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "data = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, valid in folds.split(iris, iris['species']):\n",
    "    print(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "data = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "iris = sns.load_dataset('iris')\n",
    "y = iris['species']\n",
    "iris = iris.drop(columns=\"species\")\n",
    "print(y)\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, valid in folds.split(iris, y):\n",
    "    print(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"주요-머신러닝-모델\"></a>\n",
    "# 주요 머신러닝 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"선형-회귀-모델\"></a>\n",
    "## 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "w0 = 5\n",
    "w1 = 2\n",
    "noise = np.random.randn(100, 1)\n",
    "\n",
    "x = 4* np.random.rand(100, 1)\n",
    "y = w1*x+w0+noise\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(x,y)\n",
    "\n",
    "print('w0 : ', linear_reg_model.intercept_)\n",
    "print('w1 : ', linear_reg_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_reg_model.predict(x)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"로지스틱-회귀-모델\"></a>\n",
    "## 로지스틱 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "df[\"label\"] = breast_cancer.target\n",
    "print(df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step1) train / test 으로 나누기\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# 모델 만들기\n",
    "majority_class = train[\"label\"].mode()[0]\n",
    "\n",
    "# 기준모델의 정확도 계산을 위한 데이터 생성\n",
    "y_pred = [majority_class] * len(test)\n",
    "\n",
    "# validation 데이터셋에 대한 정확도 확인\n",
    "print(\"최빈 클래스: \", majority_class)\n",
    "print(\"validation 데이터셋 정확도: \", accuracy_score(test[\"label\"], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_train = train['label']\n",
    "X_train = train.drop(columns='label')\n",
    "y_test = test['label']\n",
    "X_test = test.drop(columns='label')\n",
    "\n",
    "# 모델 생성 및 학습 시키기\n",
    "logistic = LogisticRegression(max_iter=5000) # iter 초과시 증가\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"validation 데이터셋 정확도\", logistic.score(X_test, y_test))\n",
    "print(\"validation 데이터셋의 타겟 확률\", logistic.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"결정-트리\"></a>\n",
    "## 결정 트리\n",
    "\n",
    "* criterion : 불순도 측정 지표 [default : 'gini', 'entropy']\n",
    "* max_depth : 트리 최대 깊이 [default : None]\n",
    "    * min_samples_split과 같은 옵션이 없으면 기본으로 불순도 0까지 분할\n",
    "* min_samples_split : 노드 분할에 필요한 최소 데이터수 [default : 2]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* min_samples_leaf : 말단 노드가 되기 위한 최소 데이터수 [default : 1]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* max_features : 분할에 사용할 피쳐수 [default : None]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "    * 'auto', 'sqrt', 'log2' 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(breast_cancer['data'],\n",
    "                                breast_cancer['target'],\n",
    "                                stratify=breast_cancer['target'],\n",
    "                                test_size=0.2,\n",
    "                                random_state=0\n",
    "                                )\n",
    "                    \n",
    "decisiontree = DecisionTreeClassifier(random_state=0)\n",
    "decisiontree.fit(X_train, y_train)\n",
    "\n",
    "accuracy = decisiontree.score(X_test, y_test)\n",
    "\n",
    "print(f'결정 트리 정확도 : {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"앙상블-학습\"></a>\n",
    "## 앙상블 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#load dataset\n",
    "cancer_dataset = load_breast_cancer()\n",
    "\n",
    "# cancer_dataset 그냥 찍어보니 이상하게 나옴.. dataframe화 해줘야 헸음. \n",
    "cancer_dataset_df = pd.DataFrame(cancer_dataset.data, columns=cancer_dataset.feature_names)\n",
    "cancer_dataset_df.head()\n",
    "\n",
    "\n",
    "# dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_dataset.data, cancer_dataset.target, test_size=0.2, random_state=121)\n",
    "\n",
    "\n",
    "#weak learners: logistic regression, KNN\n",
    "logistic_regression = LogisticRegression()\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "#votinng ensemble with these two weak learners\n",
    "voting_ensemble = VotingClassifier(estimators=[(\"LogisticRegression\", logistic_regression), (\"KNN\", KNN)],\n",
    "                                  voting = 'soft')\n",
    "\n",
    "\n",
    "# voting_ensemble model train/val/test\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "y_pred = voting_ensemble.predict(X_test)\n",
    "\n",
    "print(\"voting 분류기 정확도 {0:.4f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"랜덤-포레스트\"></a>\n",
    "## 랜덤 포레스트\n",
    "\n",
    "* n_estimators : 랜덤 포레스트에 사용될 트리의 수  [default : 100]\n",
    "* criterion : 불순도 측정 지표 [default : 'gini', 'entropy']\n",
    "* max_depth : 트리 최대 깊이 [default : None]\n",
    "    * min_samples_split과 같은 옵션이 없으면 기본으로 불순도 0까지 분할\n",
    "* min_samples_split : 노드 분할에 필요한 최소 데이터수 [default : 2]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* min_samples_leaf : 말단 노드가 되기 위한 최소 데이터수 [default : 1]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "* max_features : 분할에 사용할 피쳐수 [default : None]\n",
    "    * 정수형, 실수형(비율) 사용가능\n",
    "    * 'auto', 'sqrt', 'log2' 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(breast_cancer['data'],\n",
    "                                breast_cancer['target'],\n",
    "                                stratify=breast_cancer['target'],\n",
    "                                test_size=0.2,\n",
    "                                random_state=0\n",
    "                                )\n",
    "                    \n",
    "decisiontree = RandomForestClassifier(random_state=0)\n",
    "decisiontree.fit(X_train, y_train)\n",
    "\n",
    "accuracy = decisiontree.score(X_test, y_test)\n",
    "\n",
    "print(f'결정 트리 정확도 : {accuracy:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"XGBoost\"></a>\n",
    "## XGBoost\n",
    "\n",
    "for XGBoost() class\n",
    "* booster : 부스팅 알고리즘 [default : 'gbtree', 'dart', 'gblinear']\n",
    "    * gblinear : 선형 모델(성능 하)\n",
    "    * dart : 드롭아웃 적용한 gbtree\n",
    "* objective : 훈련 목적 [default : 'reg:squarederror']\n",
    "    * 회귀에서 'reg:squarederror'\n",
    "    * 확룰성 이진분류에서 'binary:logistic'\n",
    "    * softmax 사용 다중분류에서 'multi:softmax'\n",
    "    * 확률값을 구하는 다중분류에서 'multi:softprob'\n",
    "* eta : 학습률(단계별 가중치) [default : 0.3, 0~1, normally 0.001~0.1]\n",
    "* max_depth : 트리의 최대 깊이 [default : 6, normally 3~10]\n",
    "* subsample : 개별 트리에 사용할 데이터 샘플링 비율 [default : 1, normally 0.6~1]\n",
    "* colsample_bytree : 트리 훈련에 사용하는 피쳐 샘플링 비율 [default : 1, normally 0.6~1]\n",
    "* alpha : L1(lasso) 규제 값 [default : 0]\n",
    "* reg_lambda : L2(Ridge) 규제 값 [default : 1]\n",
    "* gamma(min_split_loss) : leaf node가 분할 하기위한 최소 loss[default : 0, 0~$\\infty$]\n",
    "* min_child_weight : 과적합 방지 값 [default : 1, 0~$\\infty$]\n",
    "* scale_pos_weight : 불균형 데이터 가중치 조정 값 [default : 1]\n",
    "    * 주로 $음성타깃 \\over 양성타깃$ 으로 설정\n",
    "    \n",
    "for train() method\n",
    "* params : dict type 하이퍼 파라미터 목록\n",
    "* dtrain : 훈련 데이터셋\n",
    "* num_boost_round : 부스팅 반복 횟수 [default : 10]\n",
    "    * 성능, 과적합, 시간의 상관관계가 있음\n",
    "* evals : 검증용 데이터 셋 [default : []]\n",
    "* feval : 검증용 평가지표 evals사용시 활용 [default : None]\n",
    "* maximize : feval이 높은게 좋은가? [True, False]\n",
    "* early_stopping_rounds : 조기종료 옵션 [default : None, int]\n",
    "    * eta가 크면 줄이고 작으면 키움\n",
    "    * evals가 필요함\n",
    "    * 옵션의 숫자만큼 반복하고 다음번으로 반복한 결과와 비교해 성능이 감소하면 종료\n",
    "* verbose_eval : 성능 점수 로그 설정 [default : True, False, int]\n",
    "    * 설정된 횟수만큼 반복후 성능지표를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"LightGBM\"></a>\n",
    "## LightGBM\n",
    "\n",
    "for LightGBM() class\n",
    "* boosting_type : 부스팅 알고리즘 [default : 'gbdt', 'dart', 'goss','rf]\n",
    "    * g\n",
    "* objective : 훈련 목적 [default : 'regression']\n",
    "    * 회귀에서 'regression'\n",
    "    * 이진분류에서 'binary'\n",
    "    * 다중분류에서 'multiclass'\n",
    "* learning_rate(eta) : 학습률(단계별 가중치) [default : ]\n",
    "* num_leaves : leaf node 갯수 커지면 과적합 위험 [default : 31]\n",
    "* max_depth : 트리의 최대 깊이 [default : -1]\n",
    "* bagging_fraction(subsample) : 개별 트리에 사용할 데이터 샘플링 비율 [default : not 0]\n",
    "    * g\n",
    "* feature_fraction(colsample_bytree) : 트리 훈련에 사용하는 피쳐 샘플링 비율 [default : ]\n",
    "    * g\n",
    "* lambda_l1(reg_alpha) : L1(lasso) 규제 값 [default : 0]\n",
    "* lambda_l2(reg_lambda) : L2(Ridge) 규제 값 [default : 0]\n",
    "* min_child_samples : leaf node가 되기위한 최소 데이터 수 [default : 20]\n",
    "* min_child_weight : 과적합 방지 값 [default : $1e^{-3}$, 0~$\\infty$]\n",
    "* bagging_freq : 배깅 수행 빈도 [default : 0]\n",
    "    * 몇번의 iter 마다 배깅을 할지 결정\n",
    "* force_row_wise : 메모리 효율 증가 [default : False, True]\n",
    "\n",
    "for train() method\n",
    "* params : dict type 하이퍼 파라미터 목록\n",
    "* train_set : 훈련 데이터셋\n",
    "* num_boost_round : 부스팅 반복 횟수 [default : 10]\n",
    "    * 성능, 과적합, 시간의 상관관계가 있음\n",
    "* valid_sets : 검증용 데이터 셋 [default : None]\n",
    "* feval : 검증용 평가지표 evals사용시 활용 [default : None]\n",
    "* categorical_feature : 범주형 데이터 지정옵션\n",
    "* early_stopping_rounds : 조기종료 옵션 [default : None, int]\n",
    "    * eta가 크면 줄이고 작으면 키움\n",
    "    * evals가 필요함\n",
    "    * 옵션의 숫자만큼 반복하고 다음번으로 반복한 결과와 비교해 성능이 감소하면 종료\n",
    "* verbose_eval : 성능 점수 로그 설정 [default : True, False, int]\n",
    "    * 설정된 횟수만큼 반복후 성능지표를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"하이퍼-파라미터-최적화\"></a>\n",
    "# 하이퍼 파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"그리드서치\"></a>\n",
    "## 그리드서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"랜덤서치\"></a>\n",
    "## 랜덤서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"베이지안-최적화\"></a>\n",
    "## 베이지안 최적화\n",
    "\n",
    "* init_points : 랜덤 탐색의 스텝 횟수\n",
    "* n_iter : 베이지안 실행 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization\n",
    "\n",
    "param_bounds = {'max_depth': (4, 8),\n",
    "'subsample': (0.6, 0.9),\n",
    "'colsample_bytree': (0.7, 1.0),\n",
    "'min_child_weight': (5, 7),\n",
    "'gamma': (8, 11),\n",
    "'reg_alpha': (7, 9),\n",
    "'reg_lambda': (1.1, 1.5)}\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def eval_function(max_depth, subsample, colsample_bytree, min_child_weight, gamma, reg_alpha, reg_lambda):\n",
    "  param_bounds = {'max_depth': max_depth,\n",
    "                  'subsample': subsample,\n",
    "                  'colsample_bytree': colsample_bytree,\n",
    "                  'min_child_weight': min_child_weight,\n",
    "                  'gamma': gamma,\n",
    "                  'reg_alpha': reg_alpha,\n",
    "                  'reg_lambda': reg_lambda}\n",
    "  xgb_model = xgb.train(params=params, dtrain=train, num_boost_round=2000, evals=[(valid, 'valid')], maximize=True, early_stopping_round=200)\n",
    "  best_iter = xgb_model.best_iteration\n",
    "  preds = xgb_model.predict(valid, iteration_range=(0, best_iter))\n",
    "  score = roc_auc_score(y_valid, preds)\n",
    "\n",
    "  return score\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizer = BayesianOptimization(f=eval_function, pbounds=param_bounds)\n",
    "\n",
    "optimizer.maximize(init_points=3, n_iter=10)\n",
    "\n",
    "max_params = optimzier.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b363c58d1093e891c4eaaf9273db69d0f7db3f9909d89c6197aa3afcc07f476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
